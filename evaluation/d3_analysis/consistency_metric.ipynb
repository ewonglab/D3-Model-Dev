{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32402b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import consistency\n",
    "import h5py\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057286b",
   "metadata": {},
   "source": [
    "# Calculate KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ccdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(consistency)\n",
    "\n",
    "# hyperparameters for ACME functions\n",
    "class_index = 1\n",
    "radius_count_cutoff = 0.10\n",
    "box_length = 0.1\n",
    "\n",
    "# output cache\n",
    "dataset_name = []\n",
    "dataset_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a20f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52\n"
     ]
    }
   ],
   "source": [
    "#From ground truth find index etc.\n",
    "gt_attribution = np.load('./attr_analysis/saliency_score/STAR_gt_ci1.npz')['arr_0']\n",
    "gt_attribution = np.swapaxes(gt_attribution,1,2)\n",
    "N, L, A = gt_attribution.shape\n",
    "\n",
    "datafile = ('./dataset/DeepSTARR_data.h5')\n",
    "dataset = h5py.File(datafile, 'r')\n",
    "x_test = np.array(dataset['X_test']).astype(np.float32)\n",
    "y_test = np.array(dataset['Y_test']).astype(np.float32)\n",
    "act_idx = np.argsort(y_test[:,class_index])[-N:]\n",
    "\n",
    "#check padding at end\n",
    "seq_L = x_test.shape[-1]\n",
    "if seq_L != L:\n",
    "    gt_attribution = gt_attribution[:,:seq_L,:]\n",
    "    \n",
    "#Ground Truth Consistency\n",
    "gt_X = np.swapaxes(x_test[act_idx],1,2)\n",
    "attribution_map = consistency.process_attribution_map(gt_attribution)\n",
    "unit_mask = np.sum(np.ones(gt_X.shape),axis=-1) / 4\n",
    "\n",
    "phi_1_s, phi_2_s, r_s = consistency.spherical_coordinates_process_2_trad([attribution_map], gt_X, unit_mask, radius_count_cutoff)\n",
    "LIM, box_length, box_volume, n_bins, n_bins_half = consistency.initialize_integration_2(box_length)\n",
    "entropic_information = consistency.calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range=3)\n",
    "print(entropic_information[0])\n",
    "dataset_name.append('STAR_gt_ci1')\n",
    "dataset_score.append(entropic_information[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dfa880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinuc_shuffle_ci1\n",
      "0.084\n",
      "sm_tran_ci1\n",
      "0.525\n",
      "fm_conv_ci1\n",
      "0.485\n",
      "fm_tran_ci1\n",
      "0.438\n",
      "sm_conv_ci1\n",
      "0.504\n"
     ]
    }
   ],
   "source": [
    "#Single Dataset Consistency\n",
    "for dataset in glob.glob('./dataset/*.npz'):\n",
    "    saliency_name = dataset.split('/')[-1].split('_')\n",
    "    saliency_name = saliency_name[3] + '_' + saliency_name[4]+ ('_ci%d.npz'%class_index)\n",
    "    \n",
    "    attribution_map = np.load('./attr_analysis/saliency_score/'+ saliency_name)['arr_0']\n",
    "    attribution_map = np.swapaxes(attribution_map,1,2)\n",
    "    X = np.load(dataset)['arr_0'][act_idx]\n",
    "    \n",
    "    if attribution_map.shape[1] != X.shape[1]:\n",
    "        attribution_map = attribution_map[:,:X.shape[1],:]\n",
    "    attribution_map = consistency.process_attribution_map(attribution_map)\n",
    "    \n",
    "    unit_mask = np.sum(np.ones(X.shape),axis=-1) / 4\n",
    "    phi_1_s, phi_2_s, r_s = consistency.spherical_coordinates_process_2_trad([attribution_map], X, unit_mask, radius_count_cutoff)\n",
    "    LIM, box_length, box_volume, n_bins, n_bins_half = consistency.initialize_integration_2(box_length)\n",
    "    entropic_information = consistency.calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range=3)\n",
    "    \n",
    "    dataset_name.append(saliency_name[:-4])\n",
    "    dataset_score.append(entropic_information[0])\n",
    "    \n",
    "    print(saliency_name[:-4])\n",
    "    print(entropic_information[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015f5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinuc_shuffle_ci1 + STAR_gt_ci1\n",
      "0.376\n",
      "sm_tran_ci1 + STAR_gt_ci1\n",
      "0.514\n",
      "fm_conv_ci1 + STAR_gt_ci1\n",
      "0.459\n",
      "fm_tran_ci1 + STAR_gt_ci1\n",
      "0.45\n",
      "sm_conv_ci1 + STAR_gt_ci1\n",
      "0.503\n"
     ]
    }
   ],
   "source": [
    "#Mixed Dataset Consistency\n",
    "#Single Dataset Consistency\n",
    "for dataset in glob.glob('./dataset/*.npz'):\n",
    "    saliency_name = dataset.split('/')[-1].split('_')\n",
    "    saliency_name = saliency_name[3] + '_' + saliency_name[4]+ ('_ci%d.npz'%class_index)\n",
    "    \n",
    "    attribution_map = np.load('./attr_analysis/saliency_score/'+ saliency_name)['arr_0']\n",
    "    attribution_map = np.swapaxes(attribution_map,1,2)\n",
    "    X = np.load(dataset)['arr_0'][act_idx]\n",
    "    X = np.concatenate((X,gt_X))\n",
    "    \n",
    "    if attribution_map.shape[1] != X.shape[1]:\n",
    "        attribution_map = attribution_map[:,:X.shape[1],:]\n",
    "    attribution_map = np.concatenate((attribution_map,gt_attribution))\n",
    "    attribution_map = consistency.process_attribution_map(attribution_map)\n",
    "    \n",
    "    unit_mask = np.sum(np.ones(X.shape),axis=-1) / 4\n",
    "    phi_1_s, phi_2_s, r_s = consistency.spherical_coordinates_process_2_trad([attribution_map], X, unit_mask, radius_count_cutoff)\n",
    "    LIM, box_length, box_volume, n_bins, n_bins_half = consistency.initialize_integration_2(box_length)\n",
    "    entropic_information = consistency.calculate_entropy_2(phi_1_s, phi_2_s, r_s, n_bins, box_length, box_volume, prior_range=3)\n",
    "    \n",
    "    dataset_name.append(saliency_name[:-4]+' + STAR_gt_ci1')\n",
    "    dataset_score.append(entropic_information[0])\n",
    "    \n",
    "    print(saliency_name[:-4]+' + STAR_gt_ci1')\n",
    "    print(entropic_information[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4415d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Model':dataset_name,'Entropy':dataset_score})\n",
    "df = df.sort_values('Model')\n",
    "df.to_csv('./attr_analysis/consistency_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b816dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
