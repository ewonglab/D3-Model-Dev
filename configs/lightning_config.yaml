# Lightning-specific configuration that extends the base config
# This config is optimized for PyTorch Lightning training

defaults:
  - _self_
  - model: small

# Dataset and model settings
ngpus: 1
tokens: 4

# Training configuration optimized for Lightning
training:
  batch_size: 256
  accum: 1
  n_iters: 500000
  snapshot_freq: 50000
  log_freq: 1000  # More frequent logging for Lightning
  eval_freq: 5000
  snapshot_freq_for_preemption: 10000
  weight: standard
  snapshot_sampling: True
  ema: 0.9999
  
  # Lightning-specific training options
  early_stopping:
    enabled: false
    patience: 10
    monitor: val_loss
    mode: min

# Data configuration
data:
  train: deepstarr
  valid: deepstarr
  cache_dir: data

# Graph configuration
graph:
  type: uniform
  file: data
  report_all: False

# Noise configuration  
noise:
  type: geometric
  sigma_min: 1e-4
  sigma_max: 20

# Sampling configuration
sampling:
  predictor: euler
  steps: 128
  noise_removal: True

# Evaluation configuration
eval:
  batch_size: 256
  perplexity: False
  perplexity_batch_size: 32

# Optimizer configuration
optim:
  weight_decay: 0
  optimizer: AdamW
  lr: 3e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  warmup: 2500
  grad_clip: 1.0

# Lightning trainer configuration
lightning:
  # Precision settings
  precision: bf16-mixed
  
  # Checkpointing
  checkpoint:
    save_top_k: 3
    monitor: val_loss
    mode: min
    save_last: true
    every_n_train_steps: ${training.snapshot_freq}
  
  # Logging
  logging:
    log_every_n_steps: ${training.log_freq}
    val_check_interval: ${training.eval_freq}
  
  # Performance optimizations
  performance:
    enable_progress_bar: true
    enable_model_summary: true
    sync_batchnorm: true  # For multi-GPU training
    find_unused_parameters: false
    
  # Strategy for multi-GPU training
  strategy: ddp  # Options: ddp, ddp_spawn, fsdp

# WandB logging configuration (optional)
wandb:
  enabled: false
  project: d3-dna-diffusion
  entity: null
  tags: []
  notes: ""

# Random seed for reproducibility
seed: 42

# Working directory (will be set by training script)
work_dir: null